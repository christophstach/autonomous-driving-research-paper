
@article{smith2015automated,
  title = {Automated and autonomous driving: regulation under uncertainty},
  author = {Smith, Bryant Walker and Svensson, Joakim},
  year = {2015},
  month = {4},
  day = {30},
  publisher = {ITF},
  url = {https://www.itf-oecd.org/automated-and-autonomous-driving}
}

@article{stocker2017,
  title = {Shared Automated Vehicles: Review of Business Models},
  author = {Adam Stocker, Susan Shaheen},
  year = {2017},
  month = {7},
  day = {20},
  publisher = {ITF},
  url = {https://www.itf-oecd.org/shared-automated-vehicles-review-business-models}
}

@article{doi:10.1080/01441647.2018.1494640,
  author = {Araz Taeihagh and Hazel Si Min Lim},
  title = {Governing autonomous vehicles: emerging responses for safety, liability, privacy, cybersecurity, and industry risks},
  journal = {Transport Reviews},
  volume = {39},
  number = {1},
  pages = {103-128},
  year  = {2019},
  publisher = {Routledge},
  doi = {10.1080/01441647.2018.1494640},
  url = {https://doi.org/10.1080/01441647.2018.1494640},
  eprint = {https://doi.org/10.1080/01441647.2018.1494640}
}

@booklet{EasyChair:1305,
  author = {Poondru Prithvinath Reddy},
  title = {Autonomous Car: Deployment of Reinforcement Learning in Various Autonomous Driving Applications},
  howpublished = {EasyChair Preprint no. 1305},
  year = {EasyChair, 2019}
}


// Moral Machine

@article{moralMachine,
  author = {Awad, Edmond
  and Dsouza, Sohan
  and Kim, Richard
  and Schulz, Jonathan
  and Henrich, Joseph
  and Shariff, Azim
  and Bonnefon, Jean-Fran{\c{c}}ois
  and Rahwan, Iyad},
  title = {The Moral Machine experiment},
  journal = {Nature},
  year = {2018},
  volume = {563},
  number = {7729},
  pages = {59-64},
  issn = {1476-4687},
  doi = {10.1038/s41586-018-0637-6},
  url = {https://doi.org/10.1038/s41586-018-0637-6}
}

@article{votingBasedSystem,
  author    = {Ritesh Noothigattu and
               Snehalkumar (Neil) S. Gaikwad and
               Edmond Awad and
               Sohan Dsouza and
               Iyad Rahwan and
               Pradeep Ravikumar and
               Ariel D. Procaccia},
  title     = {A Voting-Based System for Ethical Decision Making},
  journal   = {CoRR},
  volume    = {abs/1709.06692},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.06692},
  archivePrefix = {arXiv},
  eprint    = {1709.06692},
  timestamp = {Mon, 13 Aug 2018 16:48:02 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1709-06692},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{roadblocks,
  author = {Shariff, Azim
  and Bonnefon, Jean-Fran{\c{c}}ois
  and Rahwan, Iyad},
  title = {Psychological roadblocks to the adoption of self-driving vehicles},
  journal = {Nature Human Behaviour},
  year = {2017},
  volume = {1},
  number = {10},
  pages = {694-696},
  abstract = {Self-driving cars offer a bright future, but only if the public can overcome the psychological challenges that stand in the way of widespread adoption. We discuss three: ethical dilemmas, overreactions to accidents, and the opacity of the cars' decision-making algorithms -- and propose steps towards addressing them.},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0202-6},
  url = {https://doi.org/10.1038/s41562-017-0202-6}
}

@article {socialDilemma,
	author = {Bonnefon, Jean-Fran{\c c}ois and Shariff, Azim and Rahwan, Iyad},
	title = {The social dilemma of autonomous vehicles},
	volume = {352},
	number = {6293},
	pages = {1573--1576},
	year = {2016},
	doi = {10.1126/science.aaf2654},
	publisher = {American Association for the Advancement of Science},
	abstract = {When it becomes possible to program decision-making based on moral principles into machines, will self-interest or the public good predominate? In a series of surveys, Bonnefon et al. found that even though participants approve of autonomous vehicles that might sacrifice passengers to save others, respondents would prefer not to ride in such vehicles (see the Perspective by Greene). Respondents would also not approve regulations mandating self-sacrifice, and such regulations would make them less willing to buy an autonomous vehicle.Science, this issue p. 1573; see also p. 1514Autonomous vehicles (AVs) should reduce traffic accidents, but they will sometimes have to choose between two evils, such as running over pedestrians or sacrificing themselves and their passenger to save the pedestrians. Defining the algorithms that will help AVs make these moral decisions is a formidable challenge. We found that participants in six Amazon Mechanical Turk studies approved of utilitarian AVs (that is, AVs that sacrifice their passengers for the greater good) and would like others to buy them, but they would themselves prefer to ride in AVs that protect their passengers at all costs. The study participants disapprove of enforcing utilitarian regulations for AVs and would be less willing to buy such an AV. Accordingly, regulating for utilitarian algorithms may paradoxically increase casualties by postponing the adoption of a safer technology.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/352/6293/1573},
	eprint = {https://science.sciencemag.org/content/352/6293/1573.full.pdf},
	journal = {Science}
}


@article{vanet,
  author = {Ali, Ameer and C A, Dr},
  year = {2015},
  month = {01},
  pages = {},
  title = {Performance Analysis of Ad-Hoc Routing Protocols for VANET Using NS2 Simulation},
  volume = {6},
  journal = {www.ijcst.com}
}


@techreport{ec2019ethics,
  abstract = {The aim of the Guidelines is to promote Trustworthy AI. Trustworthy AI has three components, which should be met throughout the system's entire life cycle: (1) it should be lawful, complying with all applicable laws and regulations (2) it should be ethical, ensuring adherence to ethical principles and values and (3) it should be robust, both from a technical and social perspective since, even with good intentions, AI systems can cause unintentional harm. Each component in itself is necessary but not sufficient for the achievement of Trustworthy AI. Ideally, all three components work in harmony and overlap in their operation. If, in practice, tensions arise between these components, society should endeavour to align them.},
  added-at = {2019-04-09T00:11:27.000+0200},
  address = {Brussels},
  author = {{High-Level Expert Group on AI}},
  biburl = {https://www.bibsonomy.org/bibtex/2b8324eab4054c40237eba8840fe04345/meneteqel},
  institution = {European Commission},
  interhash = {93470c404e192934581de16f3d35d74c},
  intrahash = {b8324eab4054c40237eba8840fe04345},
  keywords = {European_Commission artificial_intelligence digital_ethics discrimination ethics},
  language = {eng},
  month = {4},
  timestamp = {2019-04-09T00:11:27.000+0200},
  title = {Ethics guidelines for trustworthy AI},
  type = {Report},
  url = {https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai},
  year = {2019}
}

@inproceedings{himmelsbach2008lidar,
  title={LIDAR-based 3D object perception},
  author={Himmelsbach, Michael and Mueller, Andre and L{\"u}ttel, Thorsten and W{\"u}nsche, Hans-Joachim},
  booktitle={Proceedings of 1st international workshop on cognition for technical systems},
  volume={1},
  year={2008}
}

@inproceedings{introductionToRadarSystems,
  title={Introduction to Radar Systems},
  author={Merrill Skolnik},
  year={1979}
}